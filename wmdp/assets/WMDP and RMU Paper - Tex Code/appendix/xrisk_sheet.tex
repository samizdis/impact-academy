\section{X-Risk Sheet}\label{app:xrisk_sheet}
We provide an analysis of how our paper contributes to reducing existential risk from AI, following the framework suggested by \citet{hendrycks2022xrisk}. Individual question responses do not decisively imply relevance or irrelevance to existential risk reduction.

\subsection{Long-Term Impact on Advanced AI Systems}
In this section, please analyze how this work shapes the process that will lead to advanced AI systems and how it steers the process in a safer direction.

\begin{enumerate}
\item \textbf{Overview.} How is this work intended to reduce existential risks from advanced AI systems? \\
\textbf{Answer:} This work aims to mitigate existential risks posed by the malicious use of LLMs in developing bioweapons and cyber weapons. \benchmark{} serves both as a metric for evaluating the presence of hazardous knowledge, and as a benchmark for testing unlearning methods. We aim to reduce biological malicious use, as the proliferation of bioweapons could increase the risk of a catastrophic pandemic, potentially causing civilizational collapse~\citep{gopal2023releasing}.
    


\item \textbf{Direct Effects.} If this work directly reduces existential risks, what are the main hazards, vulnerabilities, or failure modes that it directly affects? \\
\textbf{Answer:} \benchmark{} increases the barrier of entry for malicious actors to cause catastrophic harm. It decreases access to models with hazardous biological or cyber capabilities, reducing the number of malicious actors with the skill and access to engineer pandemics or launch cyberattacks on critical infrastructure (\cref{sec:dataset}).


\item \textbf{Diffuse Effects.} If this work reduces existential risks indirectly or diffusely, what are the main contributing factors that it affects? \\
\textbf{Answer:} Unlearning on \benchmark{} reduces the risks of language model aided cyberattacks, particularly from low-skilled malicious actors. Cyberattacks, particularly on critical infrastructure, could be catastrophic. They are a diffuse contributor to economic turbulence and political instability~\citep{weforum2024outlook}, which may increase the risk of great power conflict, which in turn would likely increase the probability of an existential catastrophe. 
Unlearning may be applied to prevent other hazardous properties of ML models, such as situational awareness.

\item \textbf{What's at Stake?} What is a future scenario in which this research direction could prevent the sudden, large-scale loss of life? If not applicable, what is a future scenario in which this research
direction be highly beneficial? \\
\textbf{Answer:} This directly reduces x-risks associated with the malicious use of language models in developing weapons of mass destruction~\citep{guembe2022aicyberattacks,gopal2023releasing,openaiBuildingEarly}.

\item \textbf{Result Fragility.} Do the findings rest on strong theoretical assumptions; are they not demonstrated using leading-edge tasks or models; or are the findings highly sensitive to hyperparameters? \hfill
$\square$
\item \textbf{Problem Difficulty.} Is it implausible that any practical system could ever markedly outperform humans at this task? \hfill $\boxtimes$
\item \textbf{Human Unreliability.} Does this approach strongly depend on handcrafted features, expert supervision, or human reliability? \hfill $\square$
\item \textbf{Competitive Pressures.} Does work towards this approach strongly trade off against raw intelligence, other general capabilities, or economic utility? \hfill $\square$
\end{enumerate}

\subsection{Safety-Capabilities Balance}
In this section, please analyze how this work relates to general capabilities and how it affects the balance between safety and hazards from general capabilities.

\begin{enumerate}[resume]
\item \textbf{Overview.} How does this improve safety more than it improves general capabilities? \\
\textbf{Answer:} Unlearning does not improve general capabilities; rather, it removes specific model capabilities while improving inherent model safety.

\item \textbf{Red Teaming.} What is a way in which this hastens general capabilities or the onset of x-risks? \\
\textbf{Answer:} Although \benchmark{} is constructed as a benchmark for measuring and reducing inherent model hazards, it may inadvertently serve as a roadmap for malicious use, hastening the onset of x-risks by lowering the barrier for causing catastrophe. To reduce these risks, we conduct an extensive sensitive information mitigation process (\cref{subsec:dataset-infohazard}).

\item \textbf{General Tasks.} Does this work advance progress on tasks that have been previously considered the subject of usual capabilities research? \hfill $\square$

\item \textbf{General Goals.} Does this improve or facilitate research towards general prediction, classification, state estimation, efficiency, scalability, generation, data compression, executing clear instructions, helpfulness, informativeness, reasoning, planning, researching, optimization, (self-)supervised learning, sequential decision making, recursive self-improvement, open-ended goals, models accessing the
Internet, or similar capabilities? \hfill $\square$

\item \textbf{Correlation with General Aptitude.} Is the analyzed capability known to be highly predicted by general cognitive ability or educational attainment? \hfill $\square$
                
\item \textbf{Safety via Capabilities.} Does this advance safety along with, or as a consequence of, advancing other capabilities or the study of AI? \hfill $\square$
\end{enumerate}

\subsection{Elaborations and Other Considerations}
\begin{enumerate}[resume]
\item \textbf{Other.} What clarifications or uncertainties about this work and x-risk are worth mentioning? \\
\textbf{Answer:} While unlearning is an important intervention for reducing model hazards, unlearning with may reduce the defensive, or beneficial, applications in those areas. unlearning should be complemented with other interventions that reduce risk (\cref{app:broader-impact}).
\end{enumerate}
