

\section{Broader Impacts of \benchmark{}}\label{app:broader-impact}

We reflect on how \benchmark{} comports with the broader landscape of risk mitigation strategies.

From a policy-making perspective, we hope that \benchmark{} guides the evaluation of hazards posed by ML systems, such as by informing the National Institutes of Standards and Technology's AI Risk Management Framework~\citep{nistRiskManagement, biden2023} or other frameworks. Moreover, \benchmark{} may serve as risk marker for more stringent policy action. For example, a model scoring above a particular threshold on \benchmark{} could be flagged for more comprehensive evaluation, such as human red teaming with biosecurity experts.

Furthermore, unlearning with \benchmark{} may reduce general-purpose capabilities of models in biology or cybersecurity, which could hamper their utility for defensive, or beneficial, applications in those areas. Therefore, unlearning should be complemented with other safety interventions, such as structured access (\cref{subsec:structured-access}). This is especially important for cybersecurity, as most cybersecurity knowledge may be used for both offensive and defensive purposes. For instance, AI progress could significantly enhance anomaly detection capabilities. This could aid attackers in disguising their activities to mimic normal usage patterns, but also inform critical infrastructure providers of atypical behavior that could signify an attack. 

In biosecurity, however, there exist categories of primarily offensive knowledge that may be unlearned without significant degradation to defensive capabilities. For instance, knowledge of historical bioweapons programs may be safely removed from models without significantly affecting knowledge related to countermeasure development or general-purpose biology. As a result, while both \benchmark{}-Bio and \benchmark{}-Cyber are both useful \emph{measurements} of hazardous language model capabilities, \benchmark{}-Bio may be the most useful tool for risk \emph{mitigation} via unlearning.

More broadly, there are other strategies, including non-technical strategies, that could be pursued to mitigate malicious use -- such as implementing universal screening of synthetic DNA orders to prevent the widespread access to pathogen DNA, addressing gaps in the regulation of Select Agents in the Federal Select Agent Program, and improving oversight of laboratory automation and outsourcing. 
    
   

\subsection{Limitations}\label{subsec:limitations} 

\benchmark{} consists of four-way multiple choice questions, potentially neglecting hazards that only surface in larger end-to-end evaluations. For instance, models that have memorized key biological concepts from the training data may be equally likely to do well on a particular multiple choice question as are models that have a true understanding of the underlying concept. Memorized facts may be particularly over-represented in our biological benchmark since many questions that were developed were drawn from open-access papers that were likely also included in the model's training data. In addition, multiple choice questions only test for whether the model retains hazardous knowledge; these questions do not test whether the model will reveal that information to the end-user in a helpful and timely manner during the planning or execution of a nefarious attack. To address these limitations, future work in this area could include generating questions from scientific papers that were only released after a model's training date cutoff, or using other strategies to generate questions which are difficult to search~\citep{rein2023gpqa, lala2023paperqa}.

\benchmark{} is a static benchmark which cannot anticipate the evolving landscape of cyber and biological risks, as threats continuously change and new technologies emerge. Moreover, as with any metric, scores on \benchmark{} do not capture the full extent of malicious use risk. As a result, benchmarking on only \benchmark{} may yield a false sense of model safety after unlearning. This limitation emphasizes the need for other safety benchmarks to complement \benchmark{}, especially as new risks emerge over time. For instance, benchmarks that assess open-ended conversations may be a more promising method to assess capabilities of future models.


\benchmark{} focuses on reducing risk for API-access models (\cref{sec:intro}); for models with publicly downloadable weights, unlearned information can be trivially re-introduced by malicious actors~\citep{lynch2024methods}. If open-source models reach similar capabilities to closed-source models in the future, these risks will remain unaddressed by this work. 






