\section{Conclusion}\label{sec:conclusion}
We propose a dataset, \benchmark{}, to evaluate the potential of malicious use in LLMs. \benchmark{} was developed by subject matter experts in biology, cybersecurity, and chemistry, and was filtered to remove sensitive or export-controlled information. Modern LLMs score highly on some aspects of \benchmark{}, suggesting presence of hazardous knowledge. We propose \emph{machine unlearning} as a safety intervention to reduce hazardous knowledge.%

Towards making progress on unlearning, we introduce \method{}, an unlearning method that removes hazardous knowledge without significantly compromising general model performance. \method{} also generalizes and successfully removes information from a private sensitive dataset. However, \method{} reduces accuracy on closely related fields, such as introductory virology and computer security, demonstrating the need for continued research towards improved unlearning precision. 
