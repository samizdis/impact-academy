\begin{abstract}
The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. %
To measure these risks, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. 
However, current evaluations are private and restricted to a narrow range of malicious use scenarios, limiting further research into mitigating risk.
To fill these gaps, we publicly release the \textbf{W}eapons of \textbf{M}ass \textbf{D}estruction \textbf{P}roxy (\benchmark{}) benchmark, %
a dataset of \totalquestions{} multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. \benchmark{} was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive \& export-controlled information. \benchmark{} serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for \emph{unlearning methods} to remove such hazardous knowledge. To guide progress on unlearning, we develop \method{}, a state-of-the-art unlearning method based on controlling model representations. %
\method{} reduces model performance on \benchmark{} while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. %
We release our benchmark and code publicly at \url{https://wmdp.ai}.
\end{abstract}
